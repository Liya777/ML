# K NEAREST NEIGHBOUR
KNN is used to classify both classification and regression problems. KNN is mainly used for classifying nonlinear data sets; which can not be separated using a single line.
Basically, we need to find the k value. K value means the number of nearest neighboring points that are to be considered for classification. They are selected based on the 
distance. The distance is calculated by Euclidean distance and  Manhattan distance. 

Euclidean distance: This is basically calculated using Pythagoras theorem. 
Manhattan distance: Manhattan distance is basically calculated by constructing a right triangle. 
Thus, these two methods help us to find the value of k.

Classification use case:
Suppose k=5, which means we obtained 5 points in the dataset. Suppose, in the classification dataset, 3 point belongs to class A while the other two points belong to class B, 
we can conclude that, the new point that is to be classified belongs to class A since the k value of class A is greater than class B.
The k value should be odd as it should not produce equal distribution in both the classes.

Regression use case:
In a regression use case, there won't be any classes. Suppose the k value to be 5. The average mean of the nearest 5 points is calculated and assigned to the new point for 
which the value is to be predicted. 
